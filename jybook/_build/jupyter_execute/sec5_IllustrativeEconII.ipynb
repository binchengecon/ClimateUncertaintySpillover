{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d701ea4b",
   "metadata": {},
   "source": [
    "# 5 Illustrative economy II: uncertainty decomposition\n",
    "\n",
    "This notebook presents compuations in section 8 of the paper.\n",
    "\n",
    "\n",
    "An advantage to the more structured approach implemented as smooth ambiguity is that it allows us to \"open the hood\" so to speak on uncertainty.   We build on the work of [Ricke and Caldeira (2014)](#RickeCaldeira:2014) by exploring the relative contributions of uncertainty in the carbon dynamics versus uncertainty in the temperature dynamics.  We depart from their analysis by studying the relative contributions  in the context of a decision  problem and  we include robustness to model misspecification  as a third source of uncertainty.    This latter adjustment applies primarily to the damage function specification.  \n",
    "We continue to use the social cost of carbon as a benchmark for assessing these contributions.   We perform these computations using the model developed in the previous section, although the approach we describe is applicable more generally.  \n",
    "For the uncertainty decomposition, we hold fixed the control law for  emissions, and  hence also the implied state evolution for damages, and  explore the consequences of imposing constraints on minimization over  the probabilities across the different models.\n",
    "\n",
    "Recall that we use climate sensitivity parameters from combinations of 16 models of temperature dynamics and 9 models of carbon dynamics.  \n",
    "A parameter $\\theta$ corresponds to climate-temperature model pair.  Let $\\Theta$ denote the full set of $L = 144$ pairs, and \n",
    "let $P_{j}$ for $j = 1,2,... J$ be a partition of the positive integers up to $L$.  The integer $J$ is set to 9 or 16 depending on whether we target the temperature models or the carbon.  \n",
    "For any given such partition, we solve a constrained version of the minimization problem in [section 4](sec4_IllustrativeEconIa.ipynb)  by targeting the probabilities assigned to partitions while imposing the benchmark probabilities conditioned on each partition.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{{\\overline \\omega}_j, j=1,2,..., J} &\n",
    "\\left(\\frac {\\partial  V}{\\partial  x }\\right) \\cdot\n",
    "\\sum_{j=1}^J {\\overline \\omega}_j \\sum_{\\ell \\in P_j}  \\left( {\\frac {\n",
    "\\pi_\\ell}  {\\sum_{\\ell \\in P_j} \\pi_\\ell}} \\right) \\mu(x, a \\mid \\theta_\\ell) \\cr\n",
    "&  + \\xi_a \\sum_{j=1}^J {\\overline \\omega}_j \\left(\\log {\\overline \\omega}_j - \\log {\\overline \\pi}_j\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where: ${\\overline \\pi}_j = {\\sum_{\\ell \\in P_j} \\pi_\\ell}$ and \n",
    "\n",
    "$$\n",
    "\\frac {\\pi_\\ell}  {{\\overline \\pi}_\\ell }  \\hspace{.5cm} \\ell \\in P_j\n",
    "$$\n",
    "\n",
    "are the baseline conditional probabilities for partition $j$.  We only minimize the probabilities across partitions while imposing the baseline conditional probabilities within a partition.  \n",
    "\n",
    "We impose $\\xi_r = \\infty$ when performing this minimization and let $\\xi_a = .01$ as in [section 4](sec4_IllustrativeEconIa.ipynb).  \n",
    "We perform additional calculations where we let $\\xi_r=1$ and $\\xi_a = \\infty$ in order to target damage function uncertainty rather than temperature or climate dynamics uncertainty[^1].  The two states in our problem are $x = (y,n)$, and we look for a value function of the form $V(y,n) = \\phi(y) + \\frac{(\\eta - 1)}{\\delta} n$ while imposing that ${\\tilde e} = \\epsilon(y)$.  For each partition of interest, we construct the corresponding HJB equation that supports this minimization. \n",
    "\n",
    "Since we are imposing the control law for emissions but constraining the minimization, the first-order conditions for emissions will no longer be satisfied.\n",
    "Recall formula for $\\log SCC$ from [section 4](sec4_IllustrativeEconIa.ipynb) with adjustments for uncertainty.  In the absence of optimality, the net benefit measure $MV(x)$ is not zero with the minimization constraints imposed.  Consistent with the SCC computation from the previous section, we use \n",
    "\n",
    "$$\n",
    "\\begin{align*} \n",
    " - \\frac {\\partial V}{\\partial x} (x) \\cdot {\\frac {\\partial \\mu}{\\partial e}} \\left[x, \\phi(x) \\right]  -  {\\frac 1 2}  {\\rm trace} \\left[  \\frac {\\partial^2 V}{\\partial x \\partial x'} (x) \\frac \\partial  {\\partial e} \\Sigma \\left[x, \\phi(x)  \\right] \\right].\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "for our cost contributions in the SCC decomposition.  \n",
    "\n",
    "We obtain the smallest cost measure when we preclude minimization altogether while solving for the value function and the largest one when we allow for full minimization with $\\xi_r = 1$ and $\\xi_a = .01.$  We have three intermediate cases corresponding to temperature dynamic uncertainty, climate dynamic uncertainty and damage function uncertainty.  The smallest of these measures corresponds to a full commitment to the baseline probabilities.  \n",
    "We form ratios with respect to the smallest measure, take logarithms and multiply by 100 to convert the numbers to percentages.    Importantly, we change both probabilities and value functions in this computation.   \n",
    "\n",
    "We  report the results in the figure below.\n",
    "From this figure, we see that the uncertainty adjustments in valuation account for twenty to thirty percent of the social cost of carbon.  The  contributions from temperature and carbon are essentially constant over time with the temperature uncertainty contribution being substantially larger.  The damage contribution is initially below half the total uncertainty, but this changes to more than half by the time the temperature anomaly reaches the lower threshold of 1.5 degrees Celsius.  \n",
    "\n",
    "For our uncertainty decomposition, we compute the logarithm of this expression for alternative partitions of the models.  We start by activating separately uncertainty aversion over \n",
    "<ol style=\"list-style-type:lower-roman\">\n",
    "    <li>models of carbon dynamics,</li> \n",
    "    <li>the models of temperature dynamics, and</li> \n",
    "    <li>the models or economic  damages</li>  \n",
    "</ol>    \n",
    "In each case we report the difference in logarithms between the computation using the baseline probabilities and the solutions from the constrained probability minimizations.  Importantly, we change both probabilities and value functions in this computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422cee10",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.model import solve_hjb_y, solve_hjb_y_jump, solve_baseline, minimize_g, minimize_π\n",
    "from src.utilities import find_nearest_value, solve_post_jump\n",
    "from src.simulation import simulate_me\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "import pickle\n",
    "pyo.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50bdb7",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# preparation\n",
    "ξ_w = 100_000\n",
    "ξ_a = 0.01\n",
    "ξ_r = 1.\n",
    "\n",
    "ϵ = 5.\n",
    "η = .032\n",
    "δ = .01\n",
    "\n",
    "θ_list = pd.read_csv('data/model144.csv', header=None).to_numpy()[:, 0]/1000.\n",
    "πc_o = np.ones_like(θ_list)/len(θ_list)\n",
    "\n",
    "σ_y = 1.2*np.mean(θ_list)\n",
    "y_underline = 1.5\n",
    "y_bar = 2.\n",
    "γ_1 = 1.7675/10000\n",
    "γ_2 = .0022*2\n",
    "γ_3 = np.linspace(0., 1./3, 20)\n",
    "πd_o = np.ones_like(γ_3)/len(γ_3)\n",
    "\n",
    "y_step = .01\n",
    "y_grid_long = np.arange(0., 5., y_step)\n",
    "y_grid_short = np.arange(0., y_bar+y_step, y_step)\n",
    "n_bar = find_nearest_value(y_grid_long, y_bar) \n",
    "\n",
    "# Uncertainty decomposition\n",
    "n_temp = 16\n",
    "n_carb = 9\n",
    "θ_reshape = θ_list.reshape(n_temp, n_carb)\n",
    "θ_temp = np.mean(θ_reshape, axis=1)\n",
    "θ_carb = np.mean(θ_reshape, axis=0)\n",
    "πc_o_temp = np.ones_like(θ_temp)/len(θ_temp)\n",
    "πc_o_carb = np.ones_like(θ_carb)/len(θ_carb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036617eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_jump_res = pickle.load(open(\"pre_jump_res\", \"rb\"))\n",
    "v_list = pickle.load(open(\"v_list\", \"rb\"))\n",
    "e_tilde_list = pickle.load(open(\"e_tilde_list\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaad659",
   "metadata": {},
   "source": [
    "The results are reported below.\n",
    "For  comparison we include the analogous computation when we activate an aversion to all three sources of uncertainty.\n",
    "\n",
    "\n",
    "With the penalties, $\\xi_p = 5$ and $\\xi_a = 0.01$, the contributions from temperature are essentially constant\n",
    "over time with the temperature uncertainty contribution being substantially larger. The damage\n",
    "contribution is initially well below half the total uncertainty, but this changes to more than half\n",
    "after one hundred years. It is important to remember that these computations are performed while imposing the planner’s solution for emissions and damages. So called “business-as-usual”\n",
    "simulations would change substantially this uncertainty accounting.\n",
    "\n",
    "Since the uncertainty components are not “additive,” we explore the joint impacts by parti-\n",
    "tioning the uncertainty using the three different pairings of contributions. The results are reported\n",
    "in subfigure (b). Not surprisingly, the combination of temperature and damage uncertainty has the biggest impact accounting for about three-fourths of the total uncertainty. In contrast, the combination of temperature and carbon uncertainty accounts for somewhere between one-half and\n",
    "one-third of the total uncertainty depending on how many years in the future we look at.\n",
    "\n",
    "The quantitative importance of damages will increase as we reduce $\\xi_p$. We see the $\\xi_p$ setting\n",
    "as dictating how much wiggle room a decision maker wants to entertain for the weighting of the\n",
    "alternative damage model specifications. For comparison, we set $\\xi_p = 0.3$ to match what we used\n",
    "previously for $\\xi_b$ (click the button with  $\\xi_a = 0.01$ and $\\xi_p = 0.3$) . With this change, minimizing probabilities are shifted almost entirely to the\n",
    "“extreme damage” specification, given us effectly an upper bound on the uncertainty contribution\n",
    "to the social cost of carbon. Now the overall uncertainty contribution ranges from thirty to sixty\n",
    "percent as shown in subfigure (a) with $\\xi_p = 0.3$ and $\\xi_a = 0.01$. The damage uncertainty contribution alone accounts for more\n",
    "than half of this where as the temperature and climate contributions remain about the same as\n",
    "before. Temperature and damage uncertainty taken together account for most of the uncertainty\n",
    "as reflected in subfigure (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92110f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ems_star = pre_jump_res[1][\"model_res\"]['e_tilde']\n",
    "ME_total = η / ems_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ξ_a = 0.01\n",
    "args = (δ, η, θ_list, γ_1, γ_2, γ_3, y_bar, πd_o, 100_000, 100_000, 100_000, σ_y, y_underline)\n",
    "ME_base, ratio_base = solve_baseline(y_grid_long,\n",
    "                                     n_bar,\n",
    "                                     ems_star[:n_bar + 1],\n",
    "                                     v_list[100_000], \n",
    "                                     args,\n",
    "                                     ϵ=1.,\n",
    "                                     tol=1e-8,\n",
    "                                     max_iter=500)\n",
    "\n",
    "# carbon\n",
    "print(\"--------------Carbon-----------------\")\n",
    "args_list_carb = []\n",
    "for γ_3_m in γ_3:\n",
    "    args_func = (η, δ, σ_y, y_bar, γ_1, γ_2, γ_3_m, θ_carb, πc_o_carb, 100_000, ξ_a)\n",
    "    args_iter = (y_grid_long, args_func, None, 1., 1e-8, 5_000, False)\n",
    "    args_list_carb.append(args_iter)\n",
    "\n",
    "ϕ_list_carb, ems_list_carb = solve_post_jump(y_grid_long, γ_3, solve_hjb_y, args_list_carb)\n",
    "args = (δ, η, θ_carb, γ_1, γ_2, γ_3, y_bar, πd_o, 100_000, ξ_a, 100_000, σ_y, y_underline)\n",
    "ME_carb, ratiocarb = minimize_π(y_grid_long, n_bar, ems_star[:n_bar + 1], ϕ_list_carb, args)\n",
    "\n",
    "# temperature\n",
    "print(\"-------------Temperature--------------\")\n",
    "args_list_temp = []\n",
    "for γ_3_m in γ_3:\n",
    "    args_func = (η, δ, σ_y, y_bar, γ_1, γ_2, γ_3_m, θ_temp, πc_o_temp, 100_000, ξ_a)\n",
    "    args_iter = (y_grid_long, args_func, None, 1., 1e-8, 5_000, False)\n",
    "    args_list_temp.append(args_iter)\n",
    "\n",
    "ϕ_list_temp, ems_list_temp = solve_post_jump(y_grid_long, γ_3, solve_hjb_y, args_list_temp)\n",
    "args = (δ, η, θ_temp, γ_1, γ_2, γ_3, y_bar, πd_o, 100_000, ξ_a, 100_000, σ_y, y_underline)\n",
    "ME_temp, ratiotemp = minimize_π(y_grid_long, n_bar, ems_star[:n_bar + 1], ϕ_list_temp, args)\n",
    "\n",
    "# damage\n",
    "print(\"-------------------Damage-----------------\")\n",
    "args = (δ, η, θ_list, γ_1, γ_2, γ_3, y_bar, πd_o, 1, 100_000, 100_000, σ_y, y_underline)\n",
    "ME_dmg, ratiotemp = minimize_g(y_grid_long, n_bar, ems_star[:n_bar + 1], v_list[100_000], args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac8567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_11 = np.abs(y_grid_long - 1.1).argmin()\n",
    "loc_15 = np.abs(y_grid_long - y_underline).argmin()\n",
    "ratios = [\n",
    "    np.log(ME_total[loc_11:loc_15 + 1] / ME_base[loc_11:loc_15 + 1]) * 100,\n",
    "    np.log(ME_dmg[loc_11:loc_15 + 1] / ME_base[loc_11:loc_15 + 1]) * 100,\n",
    "    np.log(ME_temp[loc_11:loc_15 + 1] / ME_base[loc_11:loc_15 + 1]) * 100,\n",
    "    np.log(ME_carb[loc_11:loc_15 + 1] / ME_base[loc_11:loc_15 + 1]) * 100,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b4cce6",
   "metadata": {},
   "source": [
    "Here, we repeat the computation for a different ambiguity aversion parameter, $\\xi_a = 0.005$ and compare it with the decompositions with $\\xi_a = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6af744",
   "metadata": {},
   "outputs": [],
   "source": [
    "ξ_a = 0.005\n",
    "pre_jump_res = pickle.load(open(f\"pre_jump_res_{ξ_a}\", \"rb\"))\n",
    "v_list = pickle.load(open(f\"v_list_{ξ_a}\", \"rb\"))\n",
    "e_tilde_list = pickle.load(open(f\"e_tilde_list_{ξ_a}\", \"rb\"))\n",
    "ems_star = pre_jump_res[1][\"model_res\"]['e_tilde']\n",
    "ME_total = η / ems_star\n",
    "# perform uncertainty decomposition\n",
    "args = (δ, η, θ_list, γ_1, γ_2, γ_3, y_bar, πd_o, 100_000, 100_000, 100_000, σ_y, y_underline)\n",
    "ME_base, ratio_base = solve_baseline(y_grid_long,\n",
    "                                     n_bar,\n",
    "                                     ems_star[:n_bar + 1],\n",
    "                                     v_list[100_000], \n",
    "                                     args,\n",
    "                                     ϵ=1.,\n",
    "                                     tol=1e-8,\n",
    "                                     max_iter=1_000)\n",
    "\n",
    "# carbon\n",
    "print(\"--------------Carbon-----------------\")\n",
    "args_list_carb = []\n",
    "for γ_3_m in γ_3:\n",
    "    args_func = (η, δ, σ_y, y_bar, γ_1, γ_2, γ_3_m, θ_carb, πc_o_carb, 100_000, ξ_a)\n",
    "    args_iter = (y_grid_long, args_func, None, 1., 1e-8, 5_000, False)\n",
    "    args_list_carb.append(args_iter)\n",
    "\n",
    "ϕ_list_carb, ems_list_carb = solve_post_jump(y_grid_long, γ_3, solve_hjb_y, args_list_carb)\n",
    "args = (δ, η, θ_carb, γ_1, γ_2, γ_3, y_bar, πd_o, 100_000, ξ_a, 100_000, σ_y, y_underline)\n",
    "ME_carb, ratiocarb = minimize_π(y_grid_long, n_bar, ems_star[:n_bar + 1], ϕ_list_carb, args)\n",
    "\n",
    "# temperature\n",
    "print(\"-------------Temperature--------------\")\n",
    "args_list_temp = []\n",
    "for γ_3_m in γ_3:\n",
    "    args_func = (η, δ, σ_y, y_bar, γ_1, γ_2, γ_3_m, θ_temp, πc_o_temp, 100_000, ξ_a)\n",
    "    args_iter = (y_grid_long, args_func, None, 1., 1e-8, 5_000, False)\n",
    "    args_list_temp.append(args_iter)\n",
    "\n",
    "ϕ_list_temp, ems_list_temp = solve_post_jump(y_grid_long, γ_3, solve_hjb_y, args_list_temp)\n",
    "args = (δ, η, θ_temp, γ_1, γ_2, γ_3, y_bar, πd_o, 100_000, ξ_a, 100_000, σ_y, y_underline)\n",
    "ME_temp, ratiotemp = minimize_π(y_grid_long, n_bar, ems_star[:n_bar + 1], ϕ_list_temp, args)\n",
    "\n",
    "# damage\n",
    "print(\"-------------------Damage-----------------\")\n",
    "args = (δ, η, θ_list, γ_1, γ_2, γ_3, y_bar, πd_o, 1, 100_000, 100_000, σ_y, y_underline)\n",
    "ME_dmg, ratiotemp = minimize_g(y_grid_long, n_bar, ems_star[:n_bar + 1], v_list[100_000], args, ϵ=0.5)\n",
    "\n",
    "# list of ratios\n",
    "ratios_0p005 = [\n",
    "    np.log(ME_total[loc_11:loc_15 + 1] / ME_base[loc_11:loc_15 + 1]) * 100,\n",
    "    np.log(ME_dmg[loc_11:loc_15 + 1] / ME_base[loc_11:loc_15 + 1]) * 100,\n",
    "    np.log(ME_temp[loc_11:loc_15 + 1] / ME_base[loc_11:loc_15 + 1]) * 100,\n",
    "    np.log(ME_carb[loc_11:loc_15 + 1] / ME_base[loc_11:loc_15 + 1]) * 100,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plots import plot13\n",
    "fig = go.Figure(layout=dict(width=800, height=500, plot_bgcolor=\"white\"))\n",
    "fig = plot13(fig, ratios, y_grid_long, y_underline)\n",
    "fig = plot13(fig, ratios_0p005, y_grid_long, y_underline)\n",
    "for i in range(int(len(fig.data)/2)):\n",
    "    fig.data[int(len(fig.data)/2) + i][\"visible\"] = False\n",
    "buttons = []\n",
    "ξ_a_s = [0.01, 0.005]\n",
    "for i in range(len(ξ_a_s)):\n",
    "    # Hide all traces\n",
    "    button = dict(method='update',\n",
    "                args=[\n",
    "                    {\n",
    "                        'visible': [False] * len(fig.data),\n",
    "                        'showlegend': [False] *len(fig.data),\n",
    "                    },\n",
    "                ],\n",
    "                label=\"ξa = {}\".format(ξ_a_s[i]))\n",
    "    # Enable the two traces we want to see\n",
    "    for j in range(int(len(fig.data)/2)):\n",
    "        button['args'][0][\"visible\"][int(len(fig.data)/2)*i + j] = True\n",
    "        button['args'][0][\"showlegend\"][int(len(fig.data)/2)*i + j] = True\n",
    "    # Add step to step list\n",
    "    buttons.append(button)\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type=\"buttons\",\n",
    "            active=0,\n",
    "            x=1.23,\n",
    "            y=0.75,\n",
    "            buttons=buttons,\n",
    "            pad={\"r\": 10, \"t\": 10, \"b\":10},\n",
    "            showactive=True\n",
    "        )\n",
    "    ])\n",
    "fig.update_yaxes(range=[0, 50])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64144e52",
   "metadata": {},
   "source": [
    "[^1]: While the robustness adjustment also applies to the climate dynamics, as we saw in the previous section, this adjustment was small relative to the ambiguity adjustment."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "macro-ann",
   "language": "python",
   "name": "macro-ann"
  },
  "source_map": [
   12,
   74,
   91,
   129,
   133,
   161,
   166,
   208,
   217,
   221,
   277,
   317
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}